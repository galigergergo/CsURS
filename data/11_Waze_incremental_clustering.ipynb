{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413bcf4e-ec89-4f35-98d2-26f5d7b65994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import math\n",
    "from datetime import date, datetime, timedelta\n",
    "from geopy.distance import great_circle\n",
    "from random import randrange\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "SHARED_PROJECT_PATH = '...'\n",
    "SHARED_PROJECT_PATH_POLY = '...'\n",
    "\n",
    "SERVER = '...'\n",
    "DATABASE = '...'\n",
    "USERNAME = '...'\n",
    "PASSWORD = '...'\n",
    "TRUSTSERVERCERT = '...'\n",
    "TRUSTEDCONN = '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5b5fa-62e0-4c1b-94b6-32834298402a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reading and preprocessing accident alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ade6e-5d3b-433a-bda6-848f93028f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw accident data\n",
    "connection_string = f'''\n",
    "    DRIVER={{ODBC Driver 18 for SQL Server}};\n",
    "    SERVER={SERVER};\n",
    "    DATABASE={DATABASE};\n",
    "    Trusted_Connection={TRUSTEDCONN};\n",
    "    UID={USERNAME};\n",
    "    PWD={PASSWORD};\n",
    "    TrustServerCertificate={TRUSTSERVERCERT};\n",
    "'''\n",
    "sql_query = f'''\n",
    "    SELECT *\n",
    "    FROM ...\n",
    "    WHERE type = 'ACCIDENT'\n",
    "'''\n",
    "conn = pyodbc.connect(connection_string)\n",
    "df_acc = pd.read_sql(sql_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382893f-1576-4f65-b4e3-e02e94ccdfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process raw accident data\n",
    "df_acc = df_acc.drop_duplicates()\n",
    "df_acc = df_acc.groupby('uuid').agg({\n",
    "    'city': 'first',\n",
    "    'confidence': 'max',\n",
    "    'nThumbsUp': 'first',\n",
    "    'street': 'first',\n",
    "    'country': 'first',\n",
    "    'subtype': 'first',\n",
    "    'roadType': 'first',\n",
    "    'reliability': 'max',\n",
    "    'magvar': 'first',\n",
    "    'reportRating': 'first',\n",
    "    'ts': 'first',\n",
    "    'geoWKT': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "df_acc['ts'] = pd.to_datetime(df_acc['ts'])\n",
    "\n",
    "df_acc['geometry'] = gpd.GeoSeries.from_wkt(df_acc['geoWKT'])\n",
    "df_acc = gpd.GeoDataFrame(df_acc, crs='EPSG:4326', geometry=df_acc.geometry).to_crs('EPSG:23700')\n",
    "df_acc.drop(columns=['geoWKT'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524819b-838a-4cc1-a32e-7d9b441709cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Budapest polygons\n",
    "gdf_poly = gpd.read_file(os.path.join(SHARED_PROJECT_PATH_POLY, 'bp_polygons_osm.geojson')).to_crs('EPSG:23700')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90371531-95c0-40f6-bf18-699cb25fb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter accidents for Budapest\n",
    "df_acc = df_acc[df_acc.within(gdf_poly.iloc[0].geometry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ea7c6-6382-4a55-ad5d-58aa89167069",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_geometry_wgs = df_acc.to_crs('EPSG:4236')['geometry']\n",
    "df_acc.loc[:, 'latitude'] = s_geometry_wgs.x\n",
    "df_acc.loc[:, 'longitude'] = s_geometry_wgs.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09259892-f49a-462f-a340-67ebe6f49e68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ST-DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8cfc4e-2354-4db1-8a74-bcb0347535e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS:\n",
    "    df={o1,o2,...,on} Set of objects\n",
    "    spatial_threshold = Maximum geographical coordinate (spatial) distance value\n",
    "    temporal_threshold = Maximum non-spatial distance value\n",
    "    min_neighbors = Minimun number of points within Eps1 and Eps2 distance\n",
    "OUTPUT:\n",
    "    C = {c1,c2,...,ck} Set of clusters\n",
    "\"\"\"\n",
    "def ST_DBSCAN(df, spatial_threshold, temporal_threshold, min_neighbors):\n",
    "    cluster_label = 0\n",
    "    NOISE = -1\n",
    "    UNMARKED = 777777\n",
    "    stack = []\n",
    "\n",
    "    # initialize each point with unmarked\n",
    "    df['cluster'] = UNMARKED\n",
    "    \n",
    "    # for each point in database\n",
    "    # for index, point in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    for index, point in df.iterrows():\n",
    "        if df.loc[index]['cluster'] == UNMARKED:\n",
    "            neighborhood = retrieve_neighbors(index, df, spatial_threshold, temporal_threshold)\n",
    "            \n",
    "            if len(neighborhood) < min_neighbors:\n",
    "                df.at[index, 'cluster'] = NOISE\n",
    "\n",
    "            else: # found a core point\n",
    "                cluster_label = cluster_label + 1\n",
    "                df.at[index, 'cluster'] = cluster_label# assign a label to core point\n",
    "\n",
    "                for neig_index in neighborhood: # assign core's label to its neighborhood\n",
    "                    df.at[neig_index, 'cluster'] = cluster_label\n",
    "                    stack.append(neig_index) # append neighborhood to stack\n",
    "                \n",
    "                while len(stack) > 0: # find new neighbors from core point neighborhood\n",
    "                    current_point_index = stack.pop()\n",
    "                    new_neighborhood = retrieve_neighbors(current_point_index, df, spatial_threshold, temporal_threshold)\n",
    "                    \n",
    "                    if len(new_neighborhood) >= min_neighbors: # current_point is a new core\n",
    "                        for neig_index in new_neighborhood:\n",
    "                            neig_cluster = df.loc[neig_index]['cluster']\n",
    "                            if (neig_cluster != NOISE) & (neig_cluster == UNMARKED): \n",
    "                                # TODO: verify cluster average before add new point\n",
    "                                df.at[neig_index, 'cluster'] = cluster_label\n",
    "                                stack.append(neig_index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def retrieve_neighbors(index_center, df, spatial_threshold, temporal_threshold):\n",
    "    neigborhood = []\n",
    "\n",
    "    center_point = df.loc[index_center]\n",
    "\n",
    "    # filter by time \n",
    "    min_time = center_point['ts'] - timedelta(minutes = temporal_threshold)\n",
    "    max_time = center_point['ts'] + timedelta(minutes = temporal_threshold)\n",
    "    df = df[(df['ts'] >= min_time) & (df['ts'] <= max_time)]\n",
    "\n",
    "    # filter by distance\n",
    "    for index, point in df.iterrows():\n",
    "        if index != index_center:\n",
    "            distance = great_circle((center_point['latitude'], center_point['longitude']), (point['latitude'], point['longitude'])).meters\n",
    "            if distance <= spatial_threshold:\n",
    "                neigborhood.append(index)\n",
    "\n",
    "    return neigborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35bc158-7d84-4a3c-a4e0-0dc95b6785f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_clusters(df_clust):\n",
    "    df_clust.sort_values(by=['ts', 'uuid'], inplace=True)\n",
    "    df_clust.rename(columns={'nThumbsUp': 'n_alerts_clustered'}, inplace=True)\n",
    "    df_clust['n_alerts_clustered'] = 1\n",
    "    df_clust.loc[df_clust[df_clust['subtype'] == ''].index, 'subtype'] = 'UNKNOWN'\n",
    "\n",
    "    # Generate the connection table between clustered alerts\n",
    "    df_groups = df_clust[df_clust['cluster'] != -1][['uuid']].copy()\n",
    "    df_groups.loc[:, 'gr_uuid'] = df_clust[df_clust['cluster'] != -1].groupby('cluster')['uuid'].transform('first')\n",
    "    \n",
    "    df_clust = gpd.GeoDataFrame(pd.concat([\n",
    "         gpd.GeoDataFrame(df_clust[df_clust['cluster'] != -1].groupby('cluster').agg({'uuid': 'first', 'city': 'first', 'confidence': 'max', 'street': 'first', 'country': 'first',\n",
    "                                                                                      'subtype': 'min', 'roadType': 'first', 'reliability': 'max', 'magvar': 'first',\n",
    "                                                                                      'reportRating': 'max', 'ts': 'first', 'geometry': 'first', 'n_alerts_clustered': 'count'}),\n",
    "                         crs='EPSG:23700'),\n",
    "        df_clust[df_clust['cluster'] == -1]\n",
    "    ]), crs='EPSG:23700').sort_values('ts').drop(columns=['cluster']).set_index('uuid')\n",
    "\n",
    "    return df_clust, df_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb89224-0ba7-4532-9612-83bd3eb57c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_date(start, end):\n",
    "    \"\"\"\n",
    "    This function will return a random datetime between two datetime \n",
    "    objects.\n",
    "    \"\"\"\n",
    "    delta = end - start\n",
    "    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds\n",
    "    random_second = randrange(int_delta)\n",
    "    return start + timedelta(seconds=random_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae983b7-b2fc-4b4c-b4cf-68de7438b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_random_test(ts_last_proc, buffer_window):\n",
    "    \"\"\"\n",
    "    Test incremental clustering with a random date\n",
    "        - check whether the incremental implementation returns the same clustering result as clustering all alerts at once\n",
    "    \n",
    "    INCREMENTAL CLUSTERING:\n",
    "        1. Cluster new alerts with a buffer window (24h empirically) from old alerts\n",
    "        2. Update old clusters:\n",
    "        2. Overwrite old clusters with new points from new data\n",
    "        3. Remove old clusters which are merged together in the new clusters (caused by new in-between data points)\n",
    "        \n",
    "    \"\"\"\n",
    "    # calculate date parameters\n",
    "    ts_first_proc = datetime.strftime(datetime.strptime(ts_last_proc, '%Y-%m-%d %H:%M:%S') - timedelta(days=1), '%Y-%m-%d %H:%M:%S')\n",
    "    ts_last_new = datetime.strftime(datetime.strptime(ts_last_proc, '%Y-%m-%d %H:%M:%S') + timedelta(days=1), '%Y-%m-%d %H:%M:%S')\n",
    "    ts_start_buff = datetime.strftime(datetime.strptime(ts_last_proc, '%Y-%m-%d %H:%M:%S') - timedelta(hours=buffer_window), '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # cut dataframe by dates\n",
    "    df_acc_old = df_acc[df_acc.ts.between(ts_first_proc, ts_last_proc)].sort_values('ts')\n",
    "    df_acc_new = df_acc[df_acc.ts.between(ts_start_buff, ts_last_new)].sort_values('ts')\n",
    "    df_acc_both = df_acc[df_acc.ts.between(ts_first_proc, ts_last_new)].sort_values('ts')\n",
    "\n",
    "    # perform ST-DBSCAN clustering on cut dataframes\n",
    "    df_acc_old = ST_DBSCAN(df_acc_old, 300, 200, 1).drop(columns=['latitude', 'longitude'])\n",
    "    df_acc_new = ST_DBSCAN(df_acc_new, 300, 200, 1).drop(columns=['latitude', 'longitude'])\n",
    "    df_acc_both = ST_DBSCAN(df_acc_both, 300, 200, 1).drop(columns=['latitude', 'longitude'])\n",
    "\n",
    "    # group dataframes by clusters\n",
    "    df_acc_old, _ = group_clusters(df_acc_old)\n",
    "    df_acc_new, df_groups_new = group_clusters(df_acc_new)\n",
    "    df_acc_both, _ = group_clusters(df_acc_both)\n",
    "    \n",
    "    # continue if no conflicts, if nothing has to be overwritten\n",
    "    if df_acc_old.n_alerts_clustered.sum() + df_acc_new[df_acc_new.ts > ts_last_proc].n_alerts_clustered.sum() == df_acc_both.n_alerts_clustered.sum():\n",
    "        return False, None,\\\n",
    "               df_acc_new[df_acc_new.ts.between(ts_last_proc, ts_last_new)].shape[0],\\\n",
    "               df_acc_new[df_acc_new.ts.between(ts_start_buff, ts_last_proc)].shape[0]\n",
    "\n",
    "    # overwrite matching old clustered alerts with new ones\n",
    "    #           matching: same uuid, different n_alerts_clustered\n",
    "    # find corresponding clusters to overwrite\n",
    "    df_acc_old_intersect = df_acc_old[df_acc_old['ts'] >= ts_start_buff][['n_alerts_clustered']]\n",
    "    l_uuids_to_overwrite = pd.merge(df_acc_new[['n_alerts_clustered']],\n",
    "                                    df_acc_old_intersect, on='uuid', how='inner')\\\n",
    "                                .query('n_alerts_clustered_x != n_alerts_clustered_y').index.to_list()\n",
    "\n",
    "    # If there is a new report between two old clusters, that might cause them to merge together\n",
    "    #           -> remove all old clusters that have been reclustered together into one new cluster\n",
    "    df_groups_to_drop = df_groups_new[df_groups_new['gr_uuid'].isin(l_uuids_to_overwrite)]\n",
    "    l_uuids_to_drop = pd.merge(df_groups_to_drop[df_groups_to_drop['gr_uuid'] != df_groups_to_drop['uuid']].set_index('uuid'),\n",
    "                               df_acc_old_intersect, on='uuid', how='inner').index.to_list()\n",
    "\n",
    "    # overwrite changed old clusters\n",
    "    df_acc_old.loc[l_uuids_to_overwrite] = df_acc_new.loc[l_uuids_to_overwrite]\n",
    "\n",
    "    # remove reclustered old clusters\n",
    "    df_acc_old.drop(l_uuids_to_drop, inplace=True)\n",
    "\n",
    "    # add new clusters\n",
    "    df_acc_clust = pd.concat([df_acc_old, df_acc_new[df_acc_new.ts > ts_last_proc]])\n",
    "    \n",
    "    # check if dfs are equal\n",
    "    if not df_acc_clust.sort_index().equals(df_acc_both.sort_index()):\n",
    "        return True, ts_last_proc,\\\n",
    "               df_acc_new[df_acc_new.ts.between(ts_last_proc, ts_last_new)].shape[0],\\\n",
    "               df_acc_new[df_acc_new.ts.between(ts_start_buff, ts_last_proc)].shape[0]\n",
    "   \n",
    "    return True, None,\\\n",
    "           df_acc_new[df_acc_new.ts.between(ts_last_proc, ts_last_new)].shape[0],\\\n",
    "           df_acc_new[df_acc_new.ts.between(ts_start_buff, ts_last_proc)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097998f-cf57-43eb-806c-f1f7b15dfd62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from dask import compute, delayed\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "d1 = datetime.strptime('1/1/2020 12:00 PM', '%m/%d/%Y %I:%M %p')\n",
    "d2 = datetime.strptime('1/1/2025 12:00 PM', '%m/%d/%Y %I:%M %p')\n",
    "\n",
    "rand_dates = [datetime.strftime(random_date(d1, d2), '%Y-%m-%d %H:%M:%S') for i in range(20000)]\n",
    "\n",
    "results = []\n",
    "for buffer_window in [18]:\n",
    "    delayed_results = [delayed(perform_random_test)(rand_date, buffer_window) for rand_date in rand_dates]\n",
    "    with ProgressBar():\n",
    "        results.append(compute(*delayed_results, scheduler=\"processes\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
